{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46e49a9",
   "metadata": {},
   "source": [
    "# 1-2 PyTorch 數值型態與基本運算\n",
    "\n",
    "##  PyTorch 數值型態"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "359e5367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "tmp_tensor = torch.tensor([1])\n",
    "float_32 = torch.FloatTensor([1.01]) # 32 bits\n",
    "float_64 = torch.DoubleTensor([1.01]) # 64 bits\n",
    "int_32 = torch.IntTensor([1]) # 32 bits\n",
    "int_64 = torch.LongTensor([1]) # 64 bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628513b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float_32 + float_32:\ttorch.float32\n",
      "float_32 * float_32:\ttorch.float32\n",
      "float_32 / float_32:\ttorch.float32\n",
      "float_32 + float_64:\ttorch.float64\n",
      "float_32 * float_64:\ttorch.float64\n",
      "float_32 / float_64:\ttorch.float64\n",
      "float_32 + int_32:\ttorch.float32\n",
      "float_32 * int_32:\ttorch.float32\n",
      "float_32 / int_32:\ttorch.float32\n",
      "float_32 + int_64:\ttorch.float32\n",
      "float_32 * int_64:\ttorch.float32\n",
      "float_32 / int_64:\ttorch.float32\n",
      "float_64 + float_32:\ttorch.float64\n",
      "float_64 * float_32:\ttorch.float64\n",
      "float_64 / float_32:\ttorch.float64\n",
      "float_64 + float_64:\ttorch.float64\n",
      "float_64 * float_64:\ttorch.float64\n",
      "float_64 / float_64:\ttorch.float64\n",
      "float_64 + int_32:\ttorch.float64\n",
      "float_64 * int_32:\ttorch.float64\n",
      "float_64 / int_32:\ttorch.float64\n",
      "float_64 + int_64:\ttorch.float64\n",
      "float_64 * int_64:\ttorch.float64\n",
      "float_64 / int_64:\ttorch.float64\n",
      "int_32 + float_32:\ttorch.float32\n",
      "int_32 * float_32:\ttorch.float32\n",
      "int_32 / float_32:\ttorch.float32\n",
      "int_32 + float_64:\ttorch.float64\n",
      "int_32 * float_64:\ttorch.float64\n",
      "int_32 / float_64:\ttorch.float64\n",
      "int_32 + int_32:\ttorch.int32\n",
      "int_32 * int_32:\ttorch.int32\n",
      "int_32 / int_32:\ttorch.float32\n",
      "int_32 + int_64:\ttorch.int64\n",
      "int_32 * int_64:\ttorch.int64\n",
      "int_32 / int_64:\ttorch.float32\n",
      "int_64 + float_32:\ttorch.float32\n",
      "int_64 * float_32:\ttorch.float32\n",
      "int_64 / float_32:\ttorch.float32\n",
      "int_64 + float_64:\ttorch.float64\n",
      "int_64 * float_64:\ttorch.float64\n",
      "int_64 / float_64:\ttorch.float64\n",
      "int_64 + int_32:\ttorch.int64\n",
      "int_64 * int_32:\ttorch.int64\n",
      "int_64 / int_32:\ttorch.float32\n",
      "int_64 + int_64:\ttorch.int64\n",
      "int_64 * int_64:\ttorch.int64\n",
      "int_64 / int_64:\ttorch.float32\n"
     ]
    }
   ],
   "source": [
    "names = ['float_32', 'float_64', 'int_32', 'int_64']\n",
    "for i, tmp1 in enumerate([float_32, float_64, int_32, int_64]):\n",
    "    for j, tmp2 in enumerate([float_32, float_64, int_32, int_64]):\n",
    "        print('{} + {}:\\t{}'.format(names[i], names[j], (tmp1+tmp2).dtype))\n",
    "        print('{} * {}:\\t{}'.format(names[i], names[j], (tmp1*tmp2).dtype))\n",
    "        print('{} / {}:\\t{}'.format(names[i], names[j], (tmp1/tmp2).dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdbf0ac",
   "metadata": {},
   "source": [
    "當你的Tensor是**參數**型態，也就是可微分求解的，這時候資料型態就不可以任意改變。<br>\n",
    "一般Torch模型參數預設是浮點數Float32。<br>\n",
    "Numpy預設的浮點數Double (Float64)，所以在使用上要特別注意這點，因為我們常常會把numpy的array轉成Torch的Tensor。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "986d7fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "mobilenet_v2 = torchvision.models.mobilenet_v2(pretrained=True)\n",
    "mobilenet_v2.eval() \n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "print(dummy_input.dtype)\n",
    "tmp = mobilenet_v2(dummy_input)\n",
    "print(tmp.dtype)\n",
    "print('---------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c7fbb9",
   "metadata": {},
   "source": [
    "如果我們沒有注意到輸入的資料格式，直接把numpy的array轉成torch tensor輸入到模型內。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b93052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n",
      "float64\n",
      "torch.Size([3, 224, 224])\n",
      "torch.float64\n",
      "torch.Size([1, 3, 224, 224])\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('example.png')\n",
    "img = cv2.resize(img, (224,224))\n",
    "img = np.array(img)/255\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "img = torch.tensor(img)\n",
    "img = img.permute(2, 0, 1)\n",
    "print(img.shape)\n",
    "print(img.dtype)\n",
    "img=img.unsqueeze(0)\n",
    "print(img.shape)\n",
    "print(img.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2e7cfda",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24996/3792770276.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmobilenet_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\mobilenetv2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\models\\mobilenetv2.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[1;31m# This exists since TorchScript doesn't support inheritance, so the superclass method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;31m# (this one) needs to have a name other than `forward` that can be accessed in a subclass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;31m# Cannot use \"squeeze\" as batch-size can be 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madaptive_avg_pool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 446\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 442\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "tmp = mobilenet_v2(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ebfcf",
   "metadata": {},
   "source": [
    "避免這類型的出錯，盡量在numpy array或是image型態轉成torch tensor時候，就指定他是```torch.FloatTensor```。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53eab3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('example.png')\n",
    "img = cv2.resize(img, (224,224))\n",
    "img = np.array(img)/255\n",
    "\n",
    "img = torch.FloatTensor(img)\n",
    "\n",
    "img = img.permute(2, 0, 1)\n",
    "img=img.unsqueeze(0)\n",
    "tmp = mobilenet_v2(img)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4562e94b",
   "metadata": {},
   "source": [
    "## PyTorch基本運算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c457f077",
   "metadata": {},
   "source": [
    "## Numpy-Like Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baedb5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元素點對點相乘(方法1:np.multiply(a,b)):\\        \n",
      "[[ 2  4  6]\n",
      " [12 15 18]]\n",
      "元素點對點相乘(方法2:a*b):\\        \n",
      "[[ 2  4  6]\n",
      " [12 15 18]]\n",
      "矩陣相乘(方法1: np.dot(a,c)):\\        \n",
      "[[22 28]\n",
      " [49 64]]\n",
      "矩陣相乘(方法2: a.dot(c)):\\        \n",
      "[[22 28]\n",
      " [49 64]]\n",
      "矩陣相乘(方法3: np.matmul(a,c)):\\        \n",
      "[[22 28]\n",
      " [49 64]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1,2,3],\n",
    "              [4,5,6]])\n",
    "b = np.array([[2,2,2],\n",
    "              [3,3,3]])\n",
    "c = np.array([[1,2],\n",
    "              [3,4],\n",
    "              [5,6]])\n",
    "print('元素點對點相乘(方法1:np.multiply(a,b)):\\\\\\\n",
    "        \\n{}'.format(np.multiply(a,b)))\n",
    "print('元素點對點相乘(方法2:a*b):\\\\\\\n",
    "        \\n{}'.format(a*b))\n",
    "print('矩陣相乘(方法1: np.dot(a,c)):\\\\\\\n",
    "        \\n{}'.format(np.dot(a,c)))\n",
    "print('矩陣相乘(方法2: a.dot(c)):\\\\\\\n",
    "        \\n{}'.format(a.dot(c)))\n",
    "print('矩陣相乘(方法3: np.matmul(a,c)):\\\\\\\n",
    "        \\n{}'.format(np.matmul(a,c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204bdc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "元素點對點相乘(a*b):\\        \n",
      "tensor([[ 2,  4,  6],\n",
      "        [12, 15, 18]])\n",
      "矩陣相乘(方法1: torch.mm(a,c)):\\        \n",
      "tensor([[22, 28],\n",
      "        [49, 64]])\n",
      "矩陣相乘(方法2: torch.matmul(a,c)):\\        \n",
      "tensor([[22, 28],\n",
      "        [49, 64]])\n",
      "矩陣相乘(方法3: a.matmul(c)):\\        \n",
      "tensor([[22, 28],\n",
      "        [49, 64]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3],\n",
    "                  [4,5,6]])\n",
    "b = torch.tensor([[2,2,2],\n",
    "                  [3,3,3]])\n",
    "c = torch.tensor([[1,2],\n",
    "                  [3,4],\n",
    "                  [5,6]])\n",
    "print('元素點對點相乘(a*b):\\\\\\\n",
    "        \\n{}'.format(a*b))\n",
    "print('矩陣相乘(方法1: torch.mm(a,c)):\\\\\\\n",
    "        \\n{}'.format( torch.mm(a,c)))\n",
    "print('矩陣相乘(方法2: torch.matmul(a,c)):\\\\\\\n",
    "        \\n{}'.format( torch.matmul(a,c)))\n",
    "print('矩陣相乘(方法3: a.matmul(c)):\\\\\\\n",
    "        \\n{}'.format( a.mm(c)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62112eb",
   "metadata": {},
   "source": [
    "看起來```torch.matmul```和```torch.mm```都是進行矩陣內積運算，那```torch.matmul```和```torch.mm```有沒有什麼差異?\n",
    "\n",
    "```torch.mm```就是一般我們高中數學學的矩陣相乘，```torch.mm(a,b)```的情況，矩陣/向量$a$要能跟矩陣/向量$b$對上<br>\n",
    "$$a\\in R^{(m\\times n)}, b\\in R^{(n\\times k)}，torch.mm(a,b)\\in R^{(m\\times k)}$$\n",
    "\n",
    "```torch.matmul```: 除了一般的矩陣內積運算外，他可以達到Python內建很奇怪的運算，叫做broadcasted運算，當兩個tensor的dimension是broadcasted，使用它時候會很有趣。<br>\n",
    "假設tensor a: $(i\\times 1\\times n\\times m)$ 和 tnesor b: $(k\\times m\\times p)$<br>\n",
    "torch.matmul(a,b) = $(i\\times k\\times n\\times p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48df7b0",
   "metadata": {},
   "source": [
    "### Torch broadcasted運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00de69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: torch.Size([1, 2, 3])\n",
      "b: torch.Size([3, 1, 3, 2])\n",
      "a * b: torch.Size([3, 1, 2, 2])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[[2,2,2],\n",
    "                   [3,3,3]]])\n",
    "print('a: {}'.format(a.shape))\n",
    "b = torch.tensor([\n",
    "    [[[1,1],\n",
    "      [1,1],\n",
    "      [1,1]]],\n",
    "    [[[2,2],\n",
    "      [2,2],\n",
    "      [2,2]]],\n",
    "    [[[3,3],\n",
    "      [3,3],\n",
    "      [3,3]]]\n",
    "])      \n",
    "print('b: {}'.format(b.shape))\n",
    "c=torch.matmul(a,b)\n",
    "print('a * b: {}'.format(c.shape))\n",
    "\n",
    "m=torch.zeros((3,1,2,2))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j,i,:,:] = torch.mm(a[i,:,:], b[j,0,:,:])\n",
    "print(((c-m)**2).sum())    \n",
    "\n",
    "\n",
    "# a = shape[2,4] -> [1,2,4]\n",
    "# b = shape[2,10] ->[2,1,10]\n",
    "# B=np.array(4,10)\n",
    "# for i in range(4):\n",
    "#     for j in range(10):\n",
    "#         if j>i:\n",
    "#             B[i,j] = dist(a[:,i], b[:,j])\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9d6f95",
   "metadata": {},
   "source": [
    "## Numpy broadcasted運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1728ae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: (1, 2, 3)\n",
      "b: (3, 1, 3, 2)\n",
      "a * b: (3, 1, 2, 2)\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[[2,2,2],\n",
    "               [3,3,3]]])\n",
    "print('a: {}'.format(a.shape))\n",
    "b = np.array([\n",
    "    [[[1,1],\n",
    "      [1,1],\n",
    "      [1,1]]],\n",
    "    [[[2,2],\n",
    "      [2,2],\n",
    "      [2,2]]],\n",
    "    [[[3,3],\n",
    "      [3,3],\n",
    "      [3,3]]]\n",
    "])      \n",
    "print('b: {}'.format(b.shape))\n",
    "c=np.matmul(a,b)\n",
    "print('a * b: {}'.format(c.shape))\n",
    "\n",
    "m=np.zeros((3,1,2,2))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j,i,:,:] = np.matmul(a[i,:,:], b[j,0,:,:])\n",
    "print(((c-m)**2).sum())    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e1d69",
   "metadata": {},
   "source": [
    "### 利用Python的broadcasted運算特性可以節省for loop的時間。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cd4316b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: torch.Size([100, 5, 10])\n",
      "b: torch.Size([200, 1, 10, 20])\n",
      "a * b: torch.Size([200, 100, 5, 20])\n",
      "計算時間:0.03949236869812012\n",
      "計算時間:1.445796012878418\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a=torch.rand((100,5,10))\n",
    "b=torch.rand((200,1,10,20))\n",
    "print('a: {}'.format(a.shape))\n",
    "print('b: {}'.format(b.shape))\n",
    "st= time.time()\n",
    "c=torch.matmul(a,b)\n",
    "print('a * b: {}'.format(c.shape))\n",
    "print('計算時間:{}'.format(time.time()-st))\n",
    "\n",
    "st= time.time()\n",
    "m=torch.zeros((200,100,5,20))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j,i,:,:] = torch.mm(a[i,:,:], b[j,0,:,:])\n",
    "print('計算時間:{}'.format(time.time()-st))\n",
    "print((c-m).abs().sum()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0e007ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: (100, 5, 10)\n",
      "b: (200, 1, 10, 20)\n",
      "a * b: (200, 100, 5, 20)\n",
      "計算時間:0.07759666442871094\n",
      "計算時間:0.22690081596374512\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "a=np.random.rand(100,5,10)\n",
    "b=np.random.rand(200,1,10,20)\n",
    "print('a: {}'.format(a.shape))\n",
    "print('b: {}'.format(b.shape))\n",
    "st= time.time()\n",
    "c=np.matmul(a,b)\n",
    "print('a * b: {}'.format(c.shape))\n",
    "print('計算時間:{}'.format(time.time()-st))\n",
    "\n",
    "st= time.time()\n",
    "m=np.zeros((200,100,5,20))\n",
    "for i in range(a.shape[0]):\n",
    "    for j in range(b.shape[0]):\n",
    "        m[j,i,:,:] = np.matmul(a[i,:,:], b[j,0,:,:])\n",
    "print('計算時間:{}'.format(time.time()-st))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf23f39",
   "metadata": {},
   "source": [
    "### 線性代數函數庫 (Torch vs Numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae976b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "tensor(9., dtype=torch.float64)\n",
      "[[ 0.22727273 -0.09090909]\n",
      " [-0.18181818  0.27272727]]\n",
      "tensor([[ 0.2273, -0.0909],\n",
      "        [-0.1818,  0.2727]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "a_np = np.array([[6.0, 2.0],\n",
    "                 [4.0, 5.0]])\n",
    "a_torch = torch.tensor(a_np)\n",
    "\n",
    "print(np.linalg.norm(a_np))\n",
    "print(torch.linalg.norm(a_torch))\n",
    "\n",
    "print(np.linalg.inv(a_np))\n",
    "print(torch.linalg.inv(a_torch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd67c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4ec284",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
